{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing?\n",
    "\n",
    "##### Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Usage of NLP?\n",
    "\n",
    "###### 1.Search Autocomplete\n",
    "\n",
    "It is another type of NLP that many people uses on a daily basis and have almost get what you expect when you are searching. This is thanks in large part to pioneers like Google, Google has been using these features in their search engines for years. This feature is also much helpful in companies website.\n",
    "\n",
    "<img src=\"search_autocomplete.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Search Autocorrect\n",
    "\n",
    "When we are typing something, we won't realize and make some mistakes while typing.If a search engine on a website won't catch those mistakes and instead show no results, then potential buyers might assume like you don't have the information or answer's they are looking for and may instead go to the competitor.\n",
    "\n",
    "We seen these when we type something wrong and Google's search engine will autocorrect your result and give a correct information about the topic.\n",
    "\n",
    "\n",
    "<img src=\"search_autocorrect.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.Machine Translation\n",
    "\n",
    "Suppose you're in china and you don't know chinese and you have to ask some address to local people but they are conversing to you in chinese then at that time Machine translation is saviour for you. One of the famous machine translation tool made by google, it will give you probably correct results always.\n",
    "\n",
    "\n",
    "<img src=\"machine_translation.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.Natural language Processing chatbot\n",
    "\n",
    "An NLP based chatbot is a computer program or artificial intelligence that communicates with a customer via textual or sound methods. Such programs are often designed to support clients on websites or via phone. \n",
    "\n",
    "\n",
    "<img src=\"chat_bot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Approach NLP Problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text preprocessing is a method to clean the text data and make it ready to feed data to the model. \n",
    "\n",
    "Text data contains noise in various forms like emotions, punctuation, numbers, shortforms, text in a different case. When we talk about Human Language then, there are different ways to say the same thing, And this is only the main problem we have to deal with because machines will not understand words, they need numbers so we need to convert text to numbers in an efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n",
      "--------------------------------------------------\n",
      " Gl\n",
      "--------------------------------------------------\n",
      "Glob\n",
      "--------------------------------------------------\n",
      "SG\n",
      "FIS Global\n",
      "['FIS', 'Global']\n",
      "['FIS ', 'lobal']\n",
      "fis global\n",
      "FIS GLOBAL\n",
      "FNIS Global\n",
      "FIS Global\n"
     ]
    }
   ],
   "source": [
    "# basic string\n",
    "a = \"FIS Global\"\n",
    "\n",
    "#Sliceing - Get the character at position 4(Here, indexing starts from 0)\n",
    "print(a[4])\n",
    "print(\"-\" * 50)\n",
    "print(a[3:6])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "#To get the output by negative indexing from -6 position to -2 position.\n",
    "print(a[-6:-2])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "#Get the results from position 2 to 6 but give result with the increment of 2.\n",
    "print(a[2:6:2])\n",
    "\n",
    "#strip() will remove whitespace in the string from begining to the end.\n",
    "a = \" FIS Global \"\n",
    "print(a.strip())\n",
    "\n",
    "#split() will split in the list of words\n",
    "a = \"FIS Global\"\n",
    "print(a.split())\n",
    "\n",
    "#split() will split the strings into substrings if it finds any instances of seprator.\n",
    "a = \"FIS Global\"\n",
    "print(a.split(\"G\"))\n",
    "\n",
    "#lower() will lowercase the words which are upper in the sentences.\n",
    "a = \"FIS Global\"\n",
    "print(a.lower())\n",
    "\n",
    "#upper() will transform lowercase into upper.\n",
    "a= \"FIS Global\"\n",
    "print(a.upper())\n",
    "\n",
    "#replace() will work like replace one string with another string.\n",
    "a = \"FIS Global\"\n",
    "print(a.replace(\"FIS\", \"FNIS\"))\n",
    "\n",
    "#String Concatenation\n",
    "a = \"FIS\"\n",
    "b = \"Global\"\n",
    "print(a +\" \"+ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text lowercase\n",
    "We do lowercase the text to reduce the size of the vocabulary of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather is too cloudy.possiblity of rain is high,today!!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_text(text): \n",
    "    return text.lower() \n",
    "  \n",
    "input_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\n",
    "lowercase_text(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove numbers\n",
    "We should either remove the numbers or convert those numbers into textual representations. We use regular expressions(re) to remove the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_num(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result \n",
    "  \n",
    "input_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(input_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove Punctuation\n",
    "\n",
    "We remove punctuations because of that we don't have different form of the same word. If we don't remove punctuations, then been, been, and been! will be treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove punctuation \n",
    "def rem_punct(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "  \n",
    "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
    "rem_punct(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is a common task in Natural Language Processing (NLP). It’s a fundamental step in both traditional NLP and Advanced Deep Learning-based architectures.\n",
    "\n",
    "###### Tokens are the building blocks of Natural Language.\n",
    "\n",
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone.', 'Welcome', 'to', 'FIS', 'Global.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### using simple text split\n",
    "text = \"Hello everyone. Welcome to FIS Global.\"\n",
    "text = text.split()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', '.', 'Welcome', 'to', 'FIS', 'Global', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "text = \"Hello everyone. Welcome to FIS Global.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.', 'Welcome to FIS Global.', 'You are part of AI/ML COE team']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "  \n",
    "text = \"Hello everyone. Welcome to FIS Global. You are part of AI/ML COE team\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove default stopwords:\n",
    "\n",
    "#### Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing nltk library\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "  \n",
    "# remove stopwords function \n",
    "def rem_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text \n",
    "  \n",
    "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
    "rem_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "### stopwards list\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stemming\n",
    "\n",
    "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
    "\n",
    "For Example: studies ---> studi\n",
    "\n",
    "             studying ---> study\n",
    "             \n",
    "             going ---> go\n",
    "             \n",
    "             \n",
    "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nltk's porter stemmer \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "stem1 = PorterStemmer() \n",
    "  \n",
    "# stem words in the list of tokenised words \n",
    "def s_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stem1.stem(word) for word in word_tokens] \n",
    "    return stems \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "s_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lemmatization\n",
    "\n",
    "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech(pos)\n",
    "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Additional Packages to perform text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Beautiful Soup \n",
    "is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
